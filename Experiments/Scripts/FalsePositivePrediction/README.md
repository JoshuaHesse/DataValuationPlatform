### Contents
- [eval_pipeline_jh.py:](eval_pipeline_jh.py) Collects performance metrics for False and True Positive retrieval algorithms for the benchmarks in [Datasets](../../Datasets). Performance is evaluated in terms of Precision (K=10%), Enrichment Factor (K=10%), BEDROC (alpha=20), calculation time (s) and Murko scaffold diversity. Outputs are saved in [Results](../../../Results), raw predictions are saved in [/Logs/eval](../../Logs/eval). Current approaches include MVS-A, CatBoost Object Importance, TracIn, DVRL, structural alerts (i.e. PAINS) and sorting by primary assay readout (top 10% most active compounds in primary screen are flagged as TPs, vice versa for FPs) (optional addition of knn shapley).
